# ML-Powered Sentiment Analyzer

A production-ready sentiment analysis system using LSTM neural networks for real-time tweet classification. Built with TensorFlow, Flask, and Docker for scalable deployment.

## ğŸŒŸ Features

- **High-Accuracy LSTM Model**: Bidirectional LSTM architecture achieving 91%+ accuracy on sentiment classification
- **Real-Time Inference API**: Flask REST API with sub-50ms p99 latency
- **Advanced NLP Preprocessing**: NLTK-based text cleaning, tokenization, and lemmatization
- **Model Quantization**: TensorFlow Lite optimization for reduced model size and faster inference
- **Production-Ready**: Dockerized deployment with health checks and monitoring
- **Scalable Architecture**: Designed to handle high-throughput inference workloads

## ğŸ“Š Model Performance

- **Accuracy**: 91%+ on test dataset
- **Dataset Size**: Trained on 150K+ tweets
- **Latency**: Sub-50ms p99 latency for real-time inference
- **Classes**: 3-class classification (Positive, Negative, Neutral)

## ğŸ—ï¸ Architecture

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py    # Text preprocessing and cleaning
â”‚   â”œâ”€â”€ model.py           # LSTM model architecture
â”‚   â””â”€â”€ optimize.py        # Model quantization utilities
â”œâ”€â”€ app.py                 # Flask REST API
â”œâ”€â”€ train.py              # Training pipeline
â”œâ”€â”€ Dockerfile            # Docker configuration
â””â”€â”€ docker-compose.yml    # Docker Compose setup
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- pip
- Docker (optional, for containerized deployment)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/VamsiMakke87/ML-Powered-Sentiment-Analyzer.git
cd ML-Powered-Sentiment-Analyzer
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Download NLTK data**
```python
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"
```

### Training the Model

1. **Prepare your dataset**
   - Place your dataset in `data/sentiment_data.csv`
   - Format: CSV with columns `text` (tweet text) and `sentiment` (0=negative, 1=neutral, 2=positive)
   - Or use the sample dataset: `data/sample_tweets.csv`

2. **Train the model**
```bash
python train.py
```

This will:
- Preprocess the text data
- Train the LSTM model
- Save the model to `models/sentiment_model.keras`
- Save the tokenizer to `models/tokenizer.pkl`
- Generate training history plots

### Running the API

#### Option 1: Local Deployment

```bash
# Using Flask development server
python app.py

# Using Gunicorn (production)
gunicorn --bind 0.0.0.0:5000 --workers 2 --timeout 120 app:app
```

#### Option 2: Docker Deployment

```bash
# Build and run with Docker Compose
docker-compose up --build

# Or build manually
docker build -t sentiment-analyzer .
docker run -p 5000:5000 -v $(pwd)/models:/app/models sentiment-analyzer
```

The API will be available at `http://localhost:5000`

## ğŸ“¡ API Endpoints

### Health Check
```bash
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true
}
```

### Single Prediction
```bash
POST /predict
Content-Type: application/json

{
  "text": "I love this product! It's amazing!"
}
```

**Response:**
```json
{
  "text": "I love this product! It's amazing!",
  "sentiment": "positive",
  "confidence": 0.95,
  "probabilities": {
    "negative": 0.02,
    "neutral": 0.03,
    "positive": 0.95
  },
  "inference_time_ms": 12.5
}
```

### Batch Prediction
```bash
POST /predict_batch
Content-Type: application/json

{
  "texts": [
    "I love this!",
    "This is terrible.",
    "It's okay, I guess."
  ]
}
```

**Response:**
```json
{
  "results": [
    {
      "text": "I love this!",
      "sentiment": "positive",
      "confidence": 0.92,
      "probabilities": {
        "negative": 0.03,
        "neutral": 0.05,
        "positive": 0.92
      }
    },
    ...
  ],
  "count": 3,
  "total_inference_time_ms": 35.7
}
```

### Model Information
```bash
GET /model_info
```

**Response:**
```json
{
  "vocab_size": 10000,
  "max_length": 100,
  "embedding_dim": 128,
  "lstm_units": 64,
  "model_path": "models/sentiment_model.keras"
}
```

## ğŸ”§ Model Optimization

Optimize the model for production with quantization:

```bash
python -c "from src.optimize import optimize_and_benchmark; import numpy as np; test_seq = np.random.randint(0, 10000, size=(10, 100)); optimize_and_benchmark('models/sentiment_model.keras', test_seq)"
```

This will generate:
- `sentiment_model_quantized_dynamic.tflite` - Dynamic range quantization
- `sentiment_model_quantized_float16.tflite` - Float16 quantization
- Performance benchmarks comparing original vs quantized models

## ğŸ“ Usage Examples

### Python Client

```python
import requests

# Single prediction
response = requests.post('http://localhost:5000/predict', json={
    'text': 'This product is amazing!'
})
result = response.json()
print(f"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})")

# Batch prediction
response = requests.post('http://localhost:5000/predict_batch', json={
    'texts': [
        'I love this!',
        'Not good at all.',
        'It is okay.'
    ]
})
results = response.json()
for item in results['results']:
    print(f"{item['text']}: {item['sentiment']}")
```

### cURL

```bash
# Single prediction
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "This is amazing!"}'

# Batch prediction
curl -X POST http://localhost:5000/predict_batch \
  -H "Content-Type: application/json" \
  -d '{"texts": ["Great!", "Terrible!", "Okay."]}'
```

## ğŸ§ª Testing

Test the preprocessing module:
```bash
python src/preprocessing.py
```

Test the model architecture:
```bash
python src/model.py
```

## ğŸ“¦ Project Structure

```
ML-Powered-Sentiment-Analyzer/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_tweets.csv      # Sample dataset
â”œâ”€â”€ models/                     # Saved models (generated after training)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py       # Text preprocessing utilities
â”‚   â”œâ”€â”€ model.py              # LSTM model implementation
â”‚   â””â”€â”€ optimize.py           # Model optimization tools
â”œâ”€â”€ app.py                    # Flask REST API
â”œâ”€â”€ train.py                  # Training pipeline
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ Dockerfile               # Docker configuration
â”œâ”€â”€ docker-compose.yml       # Docker Compose setup
â”œâ”€â”€ .env.example            # Environment variables template
â””â”€â”€ README.md               # Documentation
```

## ğŸ› ï¸ Technologies Used

- **Deep Learning**: TensorFlow 2.15, Keras
- **NLP**: NLTK (tokenization, lemmatization, stopwords)
- **Web Framework**: Flask, Flask-CORS
- **Data Processing**: NumPy, Pandas, scikit-learn
- **Model Optimization**: TensorFlow Lite, Model Quantization
- **Deployment**: Docker, Gunicorn
- **Visualization**: Matplotlib, Seaborn

## ğŸ” Environment Variables

Create a `.env` file based on `.env.example`:

```env
PORT=5000
DEBUG=False
MODEL_PATH=models/sentiment_model.keras
TOKENIZER_PATH=models/tokenizer.pkl
```

## ğŸ“ˆ Performance Optimization

The system includes several optimizations:

1. **Model Quantization**: Reduces model size by 70%+ and improves inference speed
2. **Batch Inference**: Supports batch predictions for higher throughput
3. **Caching**: Model loaded once at startup for fast inference
4. **Gunicorn Workers**: Multi-worker deployment for concurrent request handling
5. **Docker Multi-Stage Build**: Optimized container image size

## ğŸš§ Future Enhancements

- [ ] Add support for more languages (multilingual sentiment analysis)
- [ ] Implement model versioning and A/B testing
- [ ] Add Redis caching for frequent queries
- [ ] Create Kubernetes deployment manifests
- [ ] Add comprehensive test suite
- [ ] Implement model monitoring and drift detection
- [ ] Add Grafana dashboards for metrics visualization

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ‘¤ Author

**Vamsi Makke**
- GitHub: [@VamsiMakke87](https://github.com/VamsiMakke87)
- LinkedIn: [vamsi-makke](https://linkedin.com/in/vamsi-makke)
- Email: vamsimakke@gmail.com

## ğŸ™ Acknowledgments

- Trained on public Twitter sentiment datasets
- Built with TensorFlow and Flask
- Inspired by state-of-the-art NLP research

---

â­ **Star this repository if you find it helpful!**
